{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0995fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c3b398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d82f12b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems there might be a slight confusion in the name. As of current public announcements and releases, Google's model is called **Gemini 1.5 Flash**, not 2.5 Flash.\n",
      "\n",
      "**Gemini 1.5 Flash** is a **highly optimized, lightweight, and cost-effective variant of Google's Gemini 1.5 Pro model.**\n",
      "\n",
      "Here's a breakdown of what that means:\n",
      "\n",
      "1.  **Part of the Gemini 1.5 Family:** It shares the foundational architecture and advanced capabilities of Gemini 1.5 Pro, including its massive context window (up to 1 million tokens, with a preview of 2 million) and multimodal understanding.\n",
      "\n",
      "2.  **\"Flash\" Implies Optimization for Speed and Cost:**\n",
      "    *   **Speed:** It's designed to be significantly faster for many tasks, leading to lower latency, which is crucial for real-time applications and interactive experiences.\n",
      "    *   **Cost-Effectiveness:** It requires less computational power to run, making it much cheaper to use, especially for high-volume tasks.\n",
      "\n",
      "3.  **Designed for Specific Use Cases:**\n",
      "    *   While Gemini 1.5 Pro is the \"flagship\" model for highly complex reasoning and deeply nuanced understanding, Flash is optimized for situations where speed and efficiency are paramount, and the absolute highest level of complex reasoning isn't always required.\n",
      "    *   **Ideal for:**\n",
      "        *   **Chatbots and virtual assistants:** Rapid responses are critical.\n",
      "        *   **Summarization of large amounts of content:** Quickly condensing information.\n",
      "        *   **Data extraction:** Pulling specific information from documents or other data sources.\n",
      "        *   **Content generation (especially short-form):** Drafting emails, social media posts, etc.\n",
      "        *   **Building user interfaces:** Powering features where a quick AI response enhances the user experience.\n",
      "        *   **High-volume API calls:** When you need to process many requests efficiently.\n",
      "\n",
      "4.  **Multimodal Capabilities:** Like Gemini 1.5 Pro, Flash can process and understand various types of information simultaneously, including text, images, audio, and video. This allows it to analyze complex inputs like long videos or large codebases.\n",
      "\n",
      "**In essence, Gemini 1.5 Flash is Google's answer for developers and businesses who need the power and large context window of Gemini 1.5, but in a faster, more agile, and significantly more affordable package for everyday and high-volume AI applications.**\n"
     ]
    }
   ],
   "source": [
    "user_text1 = \"What is Gemini 2.5 Flash?\"\n",
    "\n",
    "response = model.generate_content(user_text1)\n",
    "reuslt = response.text\n",
    "print(reuslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad7d7da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The field of Natural Language Processing (NLP) has seen explosive growth and transformative advancements in recent years, largely driven by the power of deep learning and massive datasets. Here's a breakdown of the latest and most impactful developments:\n",
      "\n",
      "## The Reign of Large Language Models (LLMs) & The Transformer Architecture\n",
      "\n",
      "Undoubtedly, the most significant advancement is the rise and proliferation of **Large Language Models (LLMs)** based on the **Transformer architecture**.\n",
      "\n",
      "1.  **Transformer Architecture (Attention Mechanism):**\n",
      "    *   **What it is:** Introduced by Google in 2017, the Transformer architecture, with its core \"self-attention\" mechanism, revolutionized sequential data processing. Unlike recurrent neural networks (RNNs) that process words one by one, Transformers can process all words in a sequence simultaneously, making them highly parallelizable and efficient for training on massive datasets.\n",
      "    *   **Why it's important:** It allows models to weigh the importance of different words in a sentence relative to each other, capturing long-range dependencies far more effectively than previous architectures.\n",
      "\n",
      "2.  **Massive Pre-training & Foundation Models:**\n",
      "    *   **What it is:** LLMs like GPT-3/4 (OpenAI), PaLM/Gemini (Google), LLaMA/LLaMA 2 (Meta), and Claude (Anthropic) are trained on unfathomably large text datasets (trillions of words) from the internet. This unsupervised pre-training phase allows them to learn grammar, syntax, factual knowledge, and various patterns of human language.\n",
      "    *   **Why it's important:** This process creates \"Foundation Models\" – versatile, general-purpose models that can be adapted (fine-tuned) for a wide array of downstream tasks with minimal additional training, or even perform tasks with \"zero-shot\" (no examples) or \"few-shot\" (a few examples) learning.\n",
      "\n",
      "3.  **Emergent Abilities & Generative AI:**\n",
      "    *   **What it is:** As models scale in size (parameters) and training data, they exhibit \"emergent abilities\" – capabilities that weren't explicitly programmed or obvious in smaller models. These include sophisticated reasoning, code generation, creative writing, complex summarization, and instruction following.\n",
      "    *   **Why it's important:** This has led to the explosion of **Generative AI**, where models can produce highly coherent, contextually relevant, and creative text, images, audio, and even video based on simple prompts.\n",
      "\n",
      "4.  **Reinforcement Learning from Human Feedback (RLHF):**\n",
      "    *   **What it is:** This crucial technique is used to \"align\" LLMs with human preferences, making them more helpful, harmless, and honest. After initial pre-training, models generate multiple responses, which are then ranked by human annotators. This feedback is used to further fine-tune the model, teaching it to prefer better, safer, and more useful outputs.\n",
      "    *   **Why it's important:** RLHF (as seen in models like ChatGPT) dramatically improved the user experience, making LLMs more conversational, less prone to generating harmful content, and better at following complex instructions.\n",
      "\n",
      "## Beyond Core LLMs: Other Key Frontiers\n",
      "\n",
      "1.  **Multimodality:**\n",
      "    *   **What it is:** NLP is increasingly moving beyond just text. Multimodal models can understand and generate content across different modalities – text, images, audio, video. Examples include GPT-4V (vision capabilities), Gemini (natively multimodal), DALL-E, Stable Diffusion, and Midjourney (text-to-image).\n",
      "    *   **Why it's important:** This allows for richer interactions (e.g., asking questions about an image, generating a story with accompanying visuals) and opens up entirely new applications in content creation, accessibility, and human-computer interaction.\n",
      "\n",
      "2.  **Improved Reasoning and Planning:**\n",
      "    *   **What it is:** While still a challenge, techniques like **Chain-of-Thought (CoT) prompting** and **Tree-of-Thought (ToT) prompting** are helping LLMs break down complex problems into intermediate steps, leading to more accurate and logical reasoning, problem-solving, and planning.\n",
      "    *   **Why it's important:** This moves LLMs closer to truly \"thinking\" rather than just predicting the next word, enhancing their utility in tasks requiring logical deduction, mathematical problem-solving, and strategic decision-making.\n",
      "\n",
      "3.  **Efficiency and Smaller Models:**\n",
      "    *   **What it is:** Training and running large LLMs are incredibly expensive. Advancements are being made in:\n",
      "        *   **Quantization:** Reducing the precision of model weights to save memory and speed up inference.\n",
      "        *   **Distillation:** Training a smaller \"student\" model to mimic the behavior of a larger \"teacher\" model.\n",
      "        *   **Parameter-Efficient Fine-tuning (PEFT):** Techniques like LoRA (Low-Rank Adaptation) allow fine-tuning only a small fraction of a model's parameters, making customization much cheaper and faster.\n",
      "        *   **Mixture of Experts (MoE):** Models like Mixtral use sparse activation, where only a few \"expert\" sub-networks are activated for any given input, improving efficiency for very large models.\n",
      "    *   **Why it's important:** These advancements democratize access to powerful NLP models, enabling their deployment on edge devices, reducing operational costs, and fostering innovation outside of well-funded labs.\n",
      "\n",
      "4.  **Factuality and Hallucination Mitigation:**\n",
      "    *   **What it is:** LLMs are known for \"hallucinating\" or confidently generating false information. Techniques like **Retrieval-Augmented Generation (RAG)** address this by allowing LLMs to retrieve information from an external, authoritative knowledge base (like a company's internal documents or the web) *before* generating a response.\n",
      "    *   **Why it's important:** RAG significantly improves the factuality and trustworthiness of LLM outputs, making them more reliable for critical applications.\n",
      "\n",
      "5.  **Personalization and Customization:**\n",
      "    *   **What it is:** Beyond general fine-tuning, methods are emerging to personalize LLMs to individual users or specific domains without extensive re-training. This includes techniques that dynamically adapt the model's behavior based on user profiles or short interaction histories.\n",
      "    *   **Why it's important:** This allows for highly tailored experiences, making chatbots more contextually aware, assistants more helpful, and content generation more aligned with specific brand voices.\n",
      "\n",
      "6.  **Ethical NLP, Bias, and Safety:**\n",
      "    *   **What it is:** With the widespread deployment of powerful NLP models, there's intensified focus on addressing inherent biases (from training data), preventing the generation of harmful content (hate speech, misinformation), ensuring fairness, and increasing transparency. Research is actively exploring methods for detection, mitigation, and explainability (XAI).\n",
      "    *   **Why it's important:** Essential for building responsible AI systems that are safe, fair, and trustworthy for all users, and to navigate the societal impact of this technology.\n",
      "\n",
      "## Impact and Applications\n",
      "\n",
      "These advancements are fueling progress across virtually all NLP applications:\n",
      "*   **Hyper-realistic Chatbots and Virtual Assistants:** More natural, capable conversations.\n",
      "*   **Automated Content Creation:** Generating articles, marketing copy, code, scripts, etc.\n",
      "*   **Enhanced Search and Information Retrieval:** More intelligent and contextual results.\n",
      "*   **Advanced Machine Translation:** More fluent and context-aware translations.\n",
      "*   **Sophisticated Summarization:** Condensing long documents accurately and concisely.\n",
      "*   **Code Generation and Debugging:** Assisting developers with writing and fixing code.\n",
      "*   **Education:** Personalized tutoring, content creation.\n",
      "*   **Healthcare:** Summarizing medical notes, assisting with diagnoses (with human oversight).\n",
      "\n",
      "The field of NLP is experiencing a golden age, with rapid innovation continually pushing the boundaries of what machines can understand and generate with human language.\n"
     ]
    }
   ],
   "source": [
    "user_text2 = \"Tell me about the latest advancements in natural language processing.\"\n",
    "\n",
    "response = model.generate_content(user_text2)\n",
    "reuslt = response.text\n",
    "print(reuslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d925b768",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"email\" : [\"name\", \"password\"]}\n",
    "\n",
    "database = {\"riduan@example.com\" : [\"Riduan\", \"123456\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f00bc54a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'123456'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database[\"riduan@example.com\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b866538e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
